{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def istrip(text, pattern):\n",
    "    while len(text)>0:\n",
    "        if text[0] in pattern:\n",
    "            text = text[1:]\n",
    "        else:\n",
    "            break\n",
    "    while len(text)>0:\n",
    "        if text[-1] in pattern:\n",
    "            text = text[:-1]\n",
    "        else:\n",
    "            break\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import AlignedSent\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "def Make_bitext(path, reversed = False):\n",
    "    bitext = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = filter(None, (line.rstrip() for line in f))\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            lists = line.split('\\t')\n",
    "            source_text, target_text = lists[0], lists[1]\n",
    "            source_sentence = source_text.strip().split()\n",
    "            source_sentence = [istrip(source_text, ' .,?!_').lower() for source_text in source_sentence]\n",
    "            #source_sentence = list(pad_both_ends(source_sentence, n=2))\n",
    "            target_sentence = target_text.strip().split()\n",
    "            target_sentence = [istrip(target_text, ' .,?!_').lower() for target_text in target_sentence]\n",
    "            #target_sentence = list(pad_both_ends(target_sentence, n=2))\n",
    "            #print(source_sentence, target_sentence)\n",
    "            alignt_sen = AlignedSent(target_sentence, source_sentence ) if reversed else AlignedSent(source_sentence, target_sentence)\n",
    "            bitext.append(alignt_sen)\n",
    "    return bitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import IBMModel1, IBMModel2, IBMModel3, PhraseTable\n",
    "from nltk.lm.models import Laplace\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Translate_result:\n",
    "    \"\"\"_summary__\n",
    "\n",
    "    result of translte function, include:\n",
    "    - text: translated text:\n",
    "    - score: score for each word in list\n",
    "    - src: source langue code\n",
    "    - des: destinate langue code\n",
    "    \"\"\"\n",
    "    text: str\n",
    "    result: list\n",
    "    score: list\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    - score list for each word\n",
    "    - each element present for (total_score, aligment_score, n_gram_model_score)\n",
    "    \"\"\"\n",
    "    src: str\n",
    "    des: str\n",
    "\n",
    "model_dic = {'ibm1':IBMModel1, 'ibm2':IBMModel2, 'ibm3': IBMModel3}\n",
    "\n",
    "class Translate_model:\n",
    "    \"\"\"_summary_\n",
    "    An translate model make by alignment model and n_grams model\n",
    "    \"\"\"\n",
    "    def __init__(self, align_model = 'ibm2', n_order = 2):\n",
    "        \"\"\"## _summary_\n",
    "\n",
    "        ### init for translate model\n",
    "        An translate model mix by alignment model and n_grams model\n",
    "        ## Args:\n",
    "            - align_model (str, optional): align model to align word must in ['ibm1', 'ibm2', 'ibm3']. Defaults to 'ibm2'.\n",
    "            \n",
    "            - n_order (int, optional): n_order for n_grams model. Defaults to 2.\n",
    "\n",
    "        ## Raises:\n",
    "            - Exception: when pass wrong model or n_order < 1\n",
    "        \"\"\"\n",
    "        if align_model in model_dic:\n",
    "            self.align_model_base = model_dic[align_model]\n",
    "        else: \n",
    "            raise Exception(f\"your parameter '{align_model}' is not allow, it must be in ['ibm1', 'ibm2', 'ibm3']\")\n",
    "        if n_order > 0:\n",
    "            self.n_order = n_order\n",
    "        else:\n",
    "            raise Exception(f\"n_order must greater than 0\")\n",
    "        self.ngrams_model = Laplace(n_order)\n",
    "        self.ngrams_model2 = Laplace(n_order+1)\n",
    "\n",
    "    def fit(self, bitext, src = 'en', des = 'vi', epochs = 10):\n",
    "        \"\"\"## _summary_\n",
    "        \n",
    "        fit model\n",
    "        ### Args:\n",
    "            - bitext (list): list of AlignedSent\n",
    "            - src (str, optional): source language code. Defaults to 'en'.\n",
    "            - des (str, optional): destinate language code. Defaults to 'vi'.\n",
    "            - epochs (int, optional): number of iterator to train model. Defaults to 10.\n",
    "        \"\"\"\n",
    "        self.align_model = self.align_model_base(bitext, epochs)\n",
    "        self.src = src\n",
    "        self.des = des\n",
    "        target_list = [list(pad_both_ends(align_sent.mots, n=2)) for align_sent in bitext]\n",
    "        train_data, vocab = padded_everygram_pipeline(self.n_order, target_list)\n",
    "        self.ngrams_model.fit(train_data, vocab)\n",
    "        self.ngrams_model2.fit(train_data, vocab)\n",
    "\n",
    "        self.src_voca = set()\n",
    "        self.des_voca = set()\n",
    "        for align_sent in bitext:\n",
    "            for src_word in align_sent.words:\n",
    "                self.src_voca.add(src_word)\n",
    "            for des_word in align_sent.mots:\n",
    "                self.des_voca.add(des_word)\n",
    "\n",
    "    def text_preprocessing(self, text:str):\n",
    "        list_word = [istrip(word, ' .,?!_').lower() for word in text.strip().split() ]\n",
    "        return list_word\n",
    "    def Rearrangement(self, list_text):\n",
    "        preList = ['<s>']\n",
    "        #list_text = list_text + ['</s>']\n",
    "        n = len(list_text)#-1\n",
    "        for i in range(n):\n",
    "            best_score = -10\n",
    "            best_word = ''\n",
    "            for word in list_text:\n",
    "                try:\n",
    "                    score1 = self.ngrams_model.score(word, preList)\n",
    "                except:\n",
    "                    score1 = 0\n",
    "                try:\n",
    "                    score2 = self.ngrams_model2.score(word, preList)\n",
    "                except:\n",
    "                    score2 = 0\n",
    "                score = score1 + score2\n",
    "                if score>best_score:\n",
    "                    best_word = word\n",
    "                    best_score = score\n",
    "            preList.append(best_word)\n",
    "            list_text.remove(best_word)\n",
    "        if (preList[-1] == '</s>'):\n",
    "            preList = preList[:-1]\n",
    "        return preList[1:]\n",
    "    def Translate(self, source_sentence:str, alpha = 0.5, n_gram_intensity = 10, rearrange = False) -> Translate_result:\n",
    "        \"\"\"## _summary_\n",
    "        \n",
    "        Make translate for sentence.\n",
    "        ## Args:\n",
    "            - source_sentence (str): _description_\n",
    "            - alpha (float, optional): weight for alignment_model it must in range [0,1], if alpha = 1 mean that we ignore n_grams model, othewise.. Defaults to 0.5.\n",
    "            - n_gram_intensity (int, optional): multiply intensity for n_grams model, because probabilty of this kind model is smaller than alignment score . Defaults to 2.\n",
    "\n",
    "        ## Returns:\n",
    "           - Translate_result: Translate_result data class. Use <text> propety to get text result, for more information, read Translate_result class summary.\n",
    "        \"\"\"\n",
    "        if alpha > 1 or alpha < 0:\n",
    "            raise Exception(f\"alpha must in range [0,1]\")\n",
    "    \n",
    "        target_sentences = ['<s>']\n",
    "        score_list = []\n",
    "        source_sentence_list = self.text_preprocessing(source_sentence)\n",
    "        for source_word in source_sentence_list:\n",
    "            if source_word not in self.src_voca:\n",
    "                target_sentences.append(source_word)\n",
    "                score_list.append(-1)\n",
    "                continue\n",
    "            max_score = -1\n",
    "            best_translation = ''\n",
    "            pair_score = None\n",
    "            for target_word in self.des_voca:\n",
    "                alignment_score = self.align_model.translation_table[source_word][target_word]\n",
    "\n",
    "                context = target_sentences\n",
    "                try:\n",
    "                    n_gram_score_prob = self.ngrams_model.score(target_word, context=context)\n",
    "                except:\n",
    "                    n_gram_score_prob = 0\n",
    "                #print(f\"{source_word} translate to {target_word} with {alignment_score} and {n_gram_score_prob}\")\n",
    "                # phrase_table_score = self.__phrase_table.score(source_word, target_word)\n",
    "                total_score = alpha*alignment_score + alpha*n_gram_score_prob*n_gram_intensity*self.n_order\n",
    "                if total_score > max_score:\n",
    "                    max_score = total_score\n",
    "                    best_translation = target_word\n",
    "                    pair_score = (target_word, total_score, alignment_score, n_gram_score_prob)\n",
    "            target_sentences.append(best_translation)\n",
    "            score_list.append(pair_score)\n",
    "        target_sentences = target_sentences[1:]\n",
    "        if rearrange:\n",
    "            target_sentences = self.Rearrangement(target_sentences)\n",
    "        list_nor = [' '.join(word.split('_')) if '_' in word else word for word in target_sentences]\n",
    "        translate_result = Translate_result(' '.join(list_nor),target_sentences ,score_list, self.src, self.des)\n",
    "\n",
    "        return translate_result\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitex = Make_bitext('data.txt')\n",
    "translate_model = Translate_model('ibm2', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_model.fit(bitext=bitex, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = translate_model.Translate('she love him, but i hate him', 0.5, 2, rearrange=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cô ấy tình yêu anh ta nhưng tôi ghét anh ta\n",
      "[('cô_ấy', 0.33921620514946876, 0.6447192840307933, 0.005618854378024036), ('tình_yêu', 0.4394595035917922, 0.8749999999987379, 0.0006531678641410843), ('anh_ta', 0.24338121884206093, 0.48275174249695607, 0.0006684491978609625), ('nhưng', 0.5020053475920248, 0.9999999999968838, 0.0006684491978609625), ('tôi', 0.4919035322429771, 0.9797963692987884, 0.0006684491978609625), ('ghét', 0.33054667998666704, 0.6570826647861683, 0.0006684491978609625), ('anh_ta', 0.24338121884206093, 0.48275174249695607, 0.0006684491978609625)]\n"
     ]
    }
   ],
   "source": [
    "print(result.text)\n",
    "print(result.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = translate_model.Translate('i meet him at this morning', alpha=0.4, n_gram_intensity=2, rearrange=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tôi gặp anh ta tại này sáng'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trông tại anh ta'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_model.Translate('look at him', alpha=0.4, n_gram_intensity=2, rearrange=False).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cô ấy ai như'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_model.Translate('who she like', alpha=0.4, n_gram_intensity=2, rearrange=True).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bạn làm ghét anh ta'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_model.Translate( 'do you hate him?', alpha=0.4, n_gram_intensity=2, rearrange=True).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'họ làm không phải biết'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_model.Translate( 'they do not know', alpha=0.4, n_gram_intensity=2, rearrange=True).text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\drawt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "def meteor_score_wrapper(ref, hyp):\n",
    "    return meteor_score([ref], hyp)\n",
    "def sentence_gleu_wrapper(ref, hyp):\n",
    "    return sentence_gleu([ref], hyp)\n",
    "def evaluate(model, wrapper = sentence_gleu_wrapper, start = 0, take=100,):\n",
    "    score = 0\n",
    "    for i in range(start, take):\n",
    "        ref = bitex[i].mots\n",
    "        hyp = model.Translate(' '.join(bitex[i].words), alpha=0.4, n_gram_intensity=2, rearrange=True)\n",
    "        score_i = wrapper(ref, hyp.result)\n",
    "        #print(f\"source: {bitex[i].words} \\t ref: {' '.join(ref)} \\t hyp: {hyp.text} | \\t meteor score is {score_i}\" )\n",
    "\n",
    "        score += score_i\n",
    "    return score/take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.214061286324407"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(translate_model,sentence_gleu_wrapper, 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36281934111852654"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(translate_model,meteor_score_wrapper, 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_model3 = Translate_model('ibm2', 3)\n",
    "translate_model3.fit(bitext=bitex, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = translate_model3.Translate('why do she like him', alpha=0.4, n_gram_intensity=2, rearrange=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cô ấy thích tại sao tờ anh ta'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23112699839011908"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(translate_model3,sentence_gleu_wrapper, 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3903746095773352"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(translate_model3,meteor_score_wrapper, 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'không phải chắc chắn nếu bạn biết này'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_model3.Translate('Not sure if you know this', alpha=0.4, n_gram_intensity=2, rearrange=False).text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
